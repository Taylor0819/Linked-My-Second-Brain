---
index:
  - "[🏷️ AI](🏷️%20AI.md)"
  - "[🏷️ 논문](🏷️%20논문.md)"
---
*One thing that should be learned from the bitter lesson is the great power of general
purpose methods, of methods that continue to scale with increased computation
even as the available computation becomes very great. The two methods that seem
to scale arbitrarily in this way are **search** and **learning**. — Richard Sutton, 2019*

## Introduction

인공지능(AI) 분야는 최근 2년 동안 대규모 언어 모델(LLM)의 탐구와 발전이 급격히 이루어졌습니다. LLM은 프로그래밍 및 고급 수학 문제 해결과 같은 복잡한 작업을 점차 처리할 수 있게 되었습니다. OpenAI의 o1 모델은 긴 추론 과정을 생성하고 인간과 유사한 방식으로 질문을 명확히 하고 분해하며 이전의 실수를 반영 및 수정하고 실패 모드에서 새로운 솔루션을 탐색하는 등의 능력을 통해 AI의 중요한 이정표를 나타냅니다. o1 모델은 이전 LLM의 추론 능력을 초월하여 박사 수준의 성과를 달성했습니다.
o1의 성능은 강화 학습과 추론의 계산 증가에 따라 지속적으로 향상된다고 합니다. 이는 o1이 스스로 지도 학습에서 강화 학습으로의 전환과 훈련 계산과 추론 계산의 동시 증가라는 두 가지 패러다임 전환을 이끌 가능성을 시사합니다. o1은 훈련 시의 계산과 테스트 시의 사고 과정을 동시에 강화합니다.
이 논문에서는 o1에 대한 로드맵을 제시하며, 이 로드맵은 정책 초기화, 보상 설계, 탐색 및 학습 네 가지 주요 구성 요소로 이루어져 있습니다. 이 **네 가지 요소**는 o1와 같은 강력한 추론 능력을 갖춘 LLM을 구축하는 데 핵심이 됩니다. 
*정책 초기화는 LLM에 인간과 유사한 추론 행동을 부여하고, 보상 설계는 탐색 및 학습을 위한 안내 신호를 제공합니다. 탐색은 훈련 및 테스트 단계에서 고품질 솔루션을 생성하는 데 중요한 역할을 하며, 학습은 탐색을 통해 생성된 데이터를 사용하여 정책을 개선합니다. 이 과정은 전문가에 의해 수작업으로 수집된 데이터를 필요로 하지 않기 때문에 데이터 주석 비용을 줄이고 초인간 성능에 도달할 잠재성을 제공합니다.*

### 4가지 요소
- Policy Initialization : 정책 초기화는 강화 학습에서 에이전트가 환경 상태에 따른 행동을 선택하는 방식을 정의하는 중요한 과정입니다. LLM(대규모 언어 모델)은 솔루션 레벨, 단계 레벨 및 토큰 레벨의 세 가지 세부 수준에서 작동합니다. 솔루션 레벨의 행동은 전체 솔루션을 하나의 행동으로 간주하는 가장 거친 세분화이며, 단계 레벨의 행동은 개별 단계를 명시적인 행동으로 취급하는 중간 수준, 마지막으로 토큰 레벨의 행동은 각 개별 토큰을 행동으로 취급하는 가장 세밀한 세분화입니다.
- Reward Design : 보상 설계(Reward Design)는 강화 학습에서 에이전트가 의사 결정을 내리고 학습을 개선하기 위해 필요한 신호를 제공하는 중요한 단계입니다. 이 과정은 정책 초기화 이후의 단계로, 훈련 과정에서 모델이 효과적으로 배우고 행동할 수 있도록 가이드 신호를 생성하는 것을 목표로 합니다. 
	보상 설계에서 고려해야 할 주요 요소는 다음과 같습니다:

	행동 세분화(Action Granularity): 행동은 일반적으로 솔루션 수준, 단계 수준 및 토큰 수준의 세 가지 세부 수준으로 나눌 수 있습니다. 각 세분화 수준은 다양한 수준의 보상 신호와 연관될 수 있으며, 특정 작업에 적합한 보상 구조를 선택하는 것이 중요합니다.
	
	보상 신호의 희소성(Sparsity of Reward Signals): 많은 환경에서는 보상 신호가 희소하거나 아예 없는 경우가 많습니다. 예를 들어, 이야기 작성을 하는 경우와 같은 환경에서는 즉각적인 보상을 제공하는 것이 어려울 수 있습니다. 이러한 상황에서는 보상 모델을 선호 데이터로부터 학습함으로써 보상을 유도할 수 있습니다.
	
	보상 형성(Reward Shaping): 희소한 결과 보상을 밀집된 과정 보상으로 변환하기 위해, 보상 형태를 제시하는 방법이 존재합니다. 이는 에이전트가 더 적시에 유용한 보상 신호를 받을 수 있도록 하여 학습 효율성을 높이는 방법입니다.
	
- Search : 검색은 훈련 및 테스트 단계 모두에서 중요한 역할을 합니다. 훈련 시간 검색은 검색 프로세스에서 훈련 데이터를 생성하는 것을 의미합니다. 단순 샘플링 대신 검색을 사용하여 훈련 데이터를 생성하는 장점은 검색이 더 나은 행동이나 솔루션, 즉 더 높은 품질의 훈련 데이터를 제공하여 학습 효과를 향상시킨다는 것입니다. 추론 단계에서도 검색은 모델의 서브 최적 정책을 개선하는 데 중요한 역할을 계속합니다. 예를 들어, AlphaGo는 테스트 중에 성능 향상을 위해 몬테 카를로 트리 검색(Monte Carlo Tree Search, MCTS)을 사용합니다. 하지만 테스트 시간 검색을 확장하면 분포 변화로 인해 역 스케일링이 발생할 수 있습니다: 정책, 보상, 그리고 가치 모델이 한 분포에서 훈련되지만 다른 분포에서 평가됩니다.
 


#AI #LLM #ReinforcementLearning #Scaling #Search #Learning [🏷️ 논문](🏷️%20논문.md)
