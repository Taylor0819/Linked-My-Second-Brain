### 1강
1. 다음 중 자연언어처리 분야에서 주로 다루는 주제가 아닌 것은?

(1) 번역
(2) 감정 분석(sentiment analysis)
(3) 질의 응답(question answering)
**(4) 자율주행**

정답: (4)

해설: 자율주행은 대체로 컴퓨터 비전 분야에서 다루게 되는 주제라고 할 수 있습니다.


2. 다음 중 ‘학습 과정에서 모델이 전제하고 있는 가정’을 가리키는 단어로 가장 적절한 것은?

**(1) Inductive Bias**
(2) Variance
(3) Transformer
(4) Embedding

정답: (1)

해설: Inductive Bias 또는 learning bias는 학습 과정에서 모델이 전제하고 있는 가정을 말합니다.


3. 다음 중 그 내용이 MLP(Multi-Layer Perceptron)의 너비와 깊이를 충분히 늘리면 어떤 연속 함수도 표현이 가능함을 나타내는 것은?

(1) Gauss-Markov Theorem
**(2) Universal Approximation Theorem**
(3) Neyman-Pearson Lemma
(4) No Free Lunch Theorem

정답: (2)

### 2강

1. 다음 중 자연언어처리 분야에서 ‘문서들의 모음(a collection of documents)’을 가리키기 위해 사

용되는 용어는?

**(1) corpus**
(2) dictionary
(3) bag of words
(4) TF-IDF

정답: (1)

해설: corpus는 문서들의 모음을 나타냅니다. dictionary는 corpus에 등장하는 단어들에 대해 인덱스
를 매긴 데이터 구조이고, bag of words와 TF-IDF는 단어/문서를 벡터로 표현하기 위한 방식의 일
종이라고 할 수 있습니다.

2. 다음 문서를 bag of words 방식으로 나타낸다고 할 때 가장 적절한 형태라고 할 수 있는 것은?

문서: ['system', 'human', 'system', 'eps']


(1) ['system', 'human', 'system', 'eps']
(2) ['system', 'human', 'eps']
**(3) {'system': 2, 'human': 1, 'eps': 1}**
(4) {'system': 1, 'human': 1, 'eps': 1}

정답: (3)

해설: bag of words는 문서를 그 문서에 등장하는 **단어들의 빈도로 표현하는 방식**입니다. (3)의 경우
에 문서가 포함하고 있는 단어와 각 단어가 등장하는 빈도를 바르게 표시하고 있으므로 bag of
words 방식으로 문서를 표현하는 것이라고 볼 수 있습니다.


3. 다음 두 벡터 사이의 코사인 유사도를 계산한 결과로 옳은 것은?

𝐚 = (1,0)
𝐛 = (4,3)

(1) 4
(2) 1
(3) 1 / 3
(4) 4 / 5

정답: (4)
![[Pasted image 20241206141000.png]]

### 3강
1. 다음 벡터의 차원은?

[1, 2, 3]

(1) 0
(2) 1
(3) 2
**(4) 3**

정답: (4)
해설: 벡터가 속한 공간의 차원이 n일 때, 해당 벡터를 n-dimensional vector 라고 합니다.

2. 다음 LSA(Latent Semantic Analysis)에 대한 설명 중 옳지 않은 것은?

(1) LSI(Latent Semantic Indexing)라고도 한다.
(2) 단어 벡터와 문서 벡터를 만들어 내기 위한 방법의 한 종류이다.
(3) Bag of Words 행렬 또는 TF-IDF 행렬을 SVD(singular value decomposition)를 통해 분해하는 방법으로 벡터를 생성한다.
**(4) SVD 적용시 차원 축소를 위해 작은 singular value에 대응되는 벡터들을 남기고 큰 singular**
**value에 대응되는 벡터들은 버리게 된다.**

정답: (4)
해설: 큰 singular value에 대응되는 벡터들이 원래의 행렬을 더 많이 설명하고 있다고 볼 수 있기
때문에 실제로는 (4)의 내용과 반대가 됩니다.

3. 다음 중 PCA (Principal Component Analysis) 및 LSA(Latent Semantic Analysis)에 대한 설명으로 옳지 않은 것은?

(1) PCA와 LSA 모두 SVD(singular value decomposition)를 적용해서 구현할 수 있다.
**(2) LSA에서는 SVD 적용 전에 각 feature의 평균값을 빼주는 centering을 적용한다.**
(3) PCA의 목적은 데이터 포인트들의 중심(center)으로부터 변동성이 큰 방향을 나타내는
principal component를 구하려는 것이다.
(4) LSA를 적용할 경우 singular vector들이 단어와 문장을 나타내게 된다.

정답: (2)
해설: centering은 보통 PCA에서 적용하게 됩니다. PCA는 데이터의 분산을 최대화하는 방향(주성분)을 찾는 것이 목표이며, 이를 위해 데이터를 보통 평균 중심화(centering) 후 공분산 행렬에 대한 고유벡터 또는 SVD를 통해 주성분을 구한다.

### 4강
1. 다음 중 word2vec 모델에 대한 설명으로 옳지 않은 것은?

(1) word2vec 모델의 종류로 Skip-gram과 CBOW가 있다.
(2) Skip-gram의 경우 타겟 단어를 모델에 입력으로 넣었을 때 문맥 단어를 예측하도록 만드는 식으로 학습이 이루어진다.
**(3) 모델 학습시 window 크기를 작게 설정할수록 타겟 단어와 연관된 문맥 단어 수가 많아지게 된다.**
(4) 학습이 진행됨에 따라 문장 내에서 가깝게 위치하는 단어들의 벡터는 유사해지게 된다.

정답: (3)
해설: window 크기를 작게 설정할수록 타겟 단어와 연관된 문맥 단어 수는 적어지게 됩니다.

2. 다음 중 Word2Vec 모델 학습시 매번 모든 단어에 대해 softmax 확률을 계산하는 것에 드는 비용을 줄이기 위해 사용될 수 있는 기법의 이름으로 적절한 것은?

(1) Regularization
(2) Layer Normalization
**(3) Negative Sampling**
(4) CBOW

정답: (3)
Word2Vec 모델에서 단어 예측 시 softmax를 그대로 사용할 경우 모든 단어에 대해 확률을 계산해야 하므로 연산량이 매우 많다. 이를 효율화하기 위해 사용하는 대표적인 방법 중 하나가 “Negative Sampling”이다. Negative Sampling은 일부 단어만 샘플링하여 softmax 계산을 단순화하고 효율적으로 모델을 학습하도록 돕는다. 따라서 정답은 (3) Negative Sampling이다.

3. 다음과 같이 softmax 함수를 적용해서 얻은 벡터의 원소(entry) 𝑎, 𝑏, 𝑐, 𝑑에 대해 𝑎 + 𝑏 + 𝑐 + 𝑑의
값으로 적절한 것은?

![[Pasted image 20241206143127.png]]

(1) 0
(2) 12
**(3) 1**
(4) 8

정답: (3)
softmax 함수는 어떤 실수 벡터를 입력으로 받고, 각 원소에 대한 지수화(exp)를 한 뒤 총합으로 나누어 확률 분포 형태로 만듦. 즉, softmax의 출력 벡터 모든 원소의 합은 항상 1임. 
여기서 softmax([3,3,8,-2])를 구하더라도 결과 벡터 a,b,c,d의 합 a+b+c+d는 반드시 1이 됨.

4. 아래 문장에 대해 Skip-gram 모델을 학습시키기 위해 타겟 단어와 문맥 단어를 추출한다고 하자.
‘sentence’가 타겟 단어일 때 window 크기를 2로 잡고 문맥 단어를 추출한다면, 추출된 문맥 단어를 모두 나열한 결과로 가장 적절한 것은?

“training objective of Skip-gram model is to find word representations that are useful for
predicting surrounding words in sentence or document.”

(1) [‘in’, ‘or’]
**(2) [‘words, ‘in’, ‘or’, ‘document’]**
(3) [‘in’, ‘sentence’]
(4) [‘words’, ‘in’]

정답: (2)

해설: window 크기가 2이면 타겟 단어로부터 두 단어만큼 떨어진 거리 내에 있는 모든 단어들이
문맥 단어에 해당되게 됩니다.

### 5강

1. 직전 2개의 단어를 가지고 다음에 올 단어를 예측하는 모델을 만든다면 다음 중 어떤 n-gram
모델에 해당한다고 볼 수 있을까?

(1) unigram
(2) bigram
**(3) trigram**
(4) 4-gram

정답: (3)
해설: n-gram 모델은 직전 n-1개 단어를 가지고 다음에 올 단어를 예측합니다.

2. 다음 중 syntactic analogy dataset 에서 나온 샘플이라고 볼 수 있는 것은?

(1) ('man', 'king', 'woman', 'queen')
(2) ('Korea', 'Seoul', 'Japan', 'Tokyo')
(3) ('milk', 'drink', 'bread', 'eat')
**(4) (‘good', 'better', 'long', 'longer')**

정답: (4)
해설: semantic analogy dataset은 단어의 의미적인 관계를 유추하도록 데이터셋이 구성되어 있는 반면, syntactic analogy dataset은 문법적인 관계를 유추하도록 구성되어 있습니다.

3. 다음 그림이 나타내고 있는 NPLM(Neural Probabilistic Language Model) 모델에 대한 설명으로
옳지 않은 것은?

![[Pasted image 20241206221118.png]]
(1) neural network를 사용하는 n-gram 모델이라고 볼 수 있다.
(2) 𝐶(𝑤𝑡−1)은 단어 𝑤𝑡−1에 대한 임베딩(embedding)을 나타낸다.
(3) 모델은 최종적으로 각 단어에 대한 확률을 계산하게 된다.
**(4) activation function은 사용하지 않고 있다.**

정답: (4)
해설: tanh 함수가 사용되고 있는 것을 볼 수 있습니다.


### 6강
1. 다음 FastText와 word2vec 모델에 대한 설명 중 적절하지 않은 것은?

(1) word2vec 모델은 학습시에 등장하지 않은 단어에 대해서는 analogy task 등을 수행할 수 없다.
**(2) FastText 모델은 학습시에 등장하지 않은 단어의 의미에 대해서 추론이 불가능하다.**
(3) FastText 모델은 단어들을 이루고 있는 character n-gram들에 해당하는 벡터를 학습하게 된다.
(4) FastText 모델은 단어를 해당 단어를 이루는 character n-gram 벡터들의 합으로 표현한다.

정답: (2)

해설: FastText 모델은 단어를 해당 단어를 이루는 character n-gram 벡터들의 합으로 표현하기 때문에 학습시에 등장하지 않은 단어의 의미에 대해서도 추론이 가능한 경우가 있습니다.

2. 다음 중 학습시에 등장하지 않아서 dictionary에 포함되어 있지 않은 단어를 가리키는 용어로
가장 적절한 것은?

**(1) OOV**
(2) LSA
(3) n-gram
(4) morphology

정답: (1)
해설: OOV는 out-of vocabulary를 나타냅니다.

![[Pasted image 20241206221859.png]]

(1)  ![[Pasted image 20241206221937.png]]부분은 𝐳𝑔와 𝐯𝑐의 점곱(dot product)을 나타낸다.
(2) 𝐳𝑔와 𝐯𝑐가 모두 unit vector라면 두 벡터 사이의 코사인 유사도가 커질 때 𝑠(𝑤, 𝑐)가 증가한다.
(3) 𝑠(𝑤𝑡 , 𝑤𝑐 ) 값들이 커지면 목적함수 전체의 값은 증가한다.
**(4) 𝑠(𝑤𝑡 , 𝑛) 값들이 작아지면 목적함수 전체의 값은 감소한다.**

정답: (4)

해설: 𝑠(𝑤𝑡 , 𝑛) 값이 작아지면 −𝑠(𝑤𝑡 , 𝑛)값은 커지게 되는데, log는 증가함수이므로 목적함수 전체의값도 증가하게 됩니다. 참고로 𝑠(𝑤𝑡 , 𝑛) 값은 𝑤𝑡 가 포함하고 있는 character n-gram들과 negative sample에 해당하는 단어 벡터 사이의 코사인 유사도가 감소하면 작아진다고 할 수 있습니다.

### 7강
1. PMI(Pointwise Mutual Information) 값은 다음과 같이 정의된다. 관련된 설명으로 적절하지 않은것은?

𝑃𝑀𝐼(𝑥, 𝑦) = log 𝑃(𝑥, 𝑦) / 𝑃(𝑥)𝑃(𝑦)

(1) x, y가 독립이면 PMI 값은 0이 된다.
**(2) PMI 값이 0이면 x와 y가 동시에 발생할 확률은 0이라고 할 수 있다.**
(3) PMI 값이 크면 x와 y가 연관성이 있다고 생각할 수 있다.
(4) 문장에서 두 단어가 가깝게 등장하는 일이 빈번하다면, 두 단어 사이의 PMI 값 또한 상대적으로 클 것이라고 생각할 수 있다.

정답: (2)

해설: 
PMI(Pointwise Mutual Information)는 두 사건(또는 단어) x와 y의 독립성에서 얼마나 벗어나는지를 측정하는 지표
PMI 값이 0이면 ) x, y가 독립일 뿐, x와 y가 동시에 발생할 확률은 0이라고 할 수는 없습니다.
𝑃(𝑥, 𝑦) = 0이면 log를 취한 값은 −∞로 가게 됩니다.

![[Pasted image 20241206222811.png]]
(1) 이 loss function을 최소화하는 식으로 학습하면, 단어들 사이의 PMI 값을 원소로 가지는 행렬을 2개의 행렬로 분해하는 것과 유사한 결과를 낸다.
(2) 타겟 단어와 문맥 단어의 점곱(dot product)이 두 단어 사이의 PMI 값과 유사해질수록 loss function의 값은 작아진다.
**(3) 이 loss function은 두 단어가 함께 등장하는 빈도가 작은 경우를 상대적으로 더 중요하게 고려한다.**   
(4) pmi(𝑖; 𝑗) = 0인 경우, 𝑤𝑖와 ~𝑤𝑗가 직교하면 loss는 0이 된다.

정답:(3)
f(xᵢⱼ) = √xᵢⱼ 이므로, xᵢⱼ가 작을 때 f(xᵢⱼ) 값도 작아진다. 즉, 함께 등장 빈도가 낮은 단어 쌍일수록 가중치가 작아지고 중요도가 낮게 반영된다. "더 중요하게 고려한다"는 잘못된 설명이다.

3. 다음 식이 나타내고 있는 지표(metric)의 이름으로 적절한 것은?
(단, 𝑤, 𝑐는 문서에서 출현하는 단어들을 의미)

max (PMI(𝑤, 𝑐) − log 𝑘 , 0)

(1) PMI(Pointwise Mutual Information)
(2) PPMI(Positive Pointwise Mutual Information)
**(3) SPPMI(Shifted PPMI)**
(4) Spearman’s rank correlation

정답: (3)

해설: PPMI의 식이 PPMI(𝑤, 𝑐) = max(PMI(𝑤, 𝑐), 0)와 같이 정의되는데, 여기서 PMI 값이 log 𝑘 만큼 shift 되어 있는 형태이므로 Shifted PPMI 라고 합니다.

~~~
(1) PMI(Pointwise Mutual Information):  
   PMI(x, y) = log( p(x, y) / [ p(x) * p(y) ] )

   여기서 p(x, y)는 두 사건(단어) x와 y가 동시에 발생할 확률, p(x), p(y)는 각각 x, y가 단독으로 발생할 확률이다.  
   이 값이 양수이면 두 단어가 서로 독립일 때 예상하는 것보다 더 자주 함께 나타난다는 뜻이고, 음수이면 그 반대이다.

(2) PPMI(Positive Pointwise Mutual Information):  
   PPMI(x, y) = max( PMI(x, y), 0 )

   PMI 값이 음수가 나오면 0으로 바꿔서 부정적인 연관성을 제거하고, 양의 값만 취하는 변형이다.  
   즉, PPMI는 PMI에서 음수값을 제거한 형태로, 두 단어가 독립보다 더 자주 나타나는 양의 연관성만 반영한다.

(3) SPPMI(Shifted Positive PMI):  
   SPPMI(x, y) = max( PMI(x, y) - log(k), 0 )

   여기서 k는 어떤 양의 정수(또는 상수)로, PMI를 단순히 0 기준이 아니라 log(k)만큼 쉬프트한 뒤, 그 결과가 음수가 되면 0으로 조정한다.  
   이 방식은 빈도가 너무 낮은 단어 쌍이 과도하게 높은 PMI 값을 갖는 문제를 완화하고, 일정 기준(k)에 따라 PMI를 보정한다.

(4) Spearman’s rank correlation (스피어만 순위 상관계수):  
   Spearman’s ρ = 1 - ( 6 * Σ(dᵢ²) ) / [ n * (n² - 1) ]

   여기서 n은 관측값(순위쌍)의 개수, dᵢ는 각 관측 i에서 순위 차이이다.  
   Spearman’s ρ는 순위 기반 상관 계수로, 두 변수 간의 단조(monotonic) 관계를 측정한다.

~~~

### 8강
1. 다음 Doc2Vec에 대한 설명 중 적절하지 않은 것은?

(1) 문서 벡터를 만들어내기 위한 모델이다.
(2) Doc2Vec 모델을 학습시키면 문서 벡터 외에 단어 벡터도 같이 얻을 수 있다.
(3) Doc2Vec 모델의 종류에는 PV-DM과 PV-DBOW가 있다.
**(4) 모델 학습 후 training dataset에서 등장하지 않은 문서에 대한 벡터는 생성할 수 없다.**

정답: (4)

해설: Doc2Vec 모델의 아이디어는 기존 neural network 단어 학습 모델에 단어와 문서 벡터 학습을 동시에 하도록 만드는 것이기 때문에, 문서 벡터 외에 단어 벡터도 얻을 수 있으며, 새롭게 주어진 문서의 벡터는 기존에 학습된 weight들을 고정시킨 채로 새로운 문서에 해당하는 벡터를 학습시킴으로써 얻어낼 수 있습니다.

2. 다음 그림이 나타내고 있는 모델로 적절한 것은?
![[Pasted image 20241206223607.png]]
**(1) PV-DM(paragraph vector with distributed memory)**
(2) PV-DBOW(paragraph vector with distributed bag of words)
(3) Word2Vec
(4) FastText

정답: (1)

해설: PV-DM은 기본적으로는 NPLM(Neural Probabilistic Language Model)과 똑같은 방식으로 학습을
하되, 단어들이 속한 문단 또는 문서에 해당하는 벡터를 단어 벡터와 같이 학습하는 방식입니다.

3. 다음 중 문서 벡터를 직접 만들어내는 모델이 아닌 것은?

(1) Doc2Vec
(2) TF-IDF
(3) LSA(Latent Semantic Analysis)
**(4) FastText**

정답: (4)

해설: 
1) Doc2Vec: Word2Vec를 확장한 방식으로, 문서(문맥) 벡터를 함께 학습함으로써 각 문서에 대한 임베딩 벡터를 직접 얻을 수 있음. 즉, 문서 벡터를 직접 만든다.
2) TF-IDF: 문서 내 단어 빈도-역문서 빈도(term frequency-inverse document frequency)를 이용해 문서를 벡터(고차원 희소 벡터)로 나타내는 대표적인 전통적 방법. 비록 학습된 임베딩 형태는 아니지만, 이 역시 문서 단위로 벡터를 직접 형성한다. 즉, 문서에 대한 벡터 표현을 직접 얻을 수 있다.
3) LSA(Latent Semantic Analysis): 단어-문서 행렬을 SVD 분해하여 낮은 차원의 잠재적 의미 공간을 구한다. 이 과정에서 문서 벡터를 직접적으로 제공한다. 즉, 각 문서는 LSA에 의해 얻어진 잠재 의미 공간에서의 좌표(벡터)를 가지게 됨.
4) FastText: 주로 단어 임베딩(단어 벡터)을 얻는 모델이다. FastText 자체는 문서나 문장 레벨의 벡터를 "직접" 만들어내는 모델이 아니다. 물론 단어 벡터의 평균 등 단순한 연산을 통해 문서 벡터를 구할 수 있지만, 이것은 모델의 직접적인 출력이 아니라 사후 처리의 결과일 뿐이다.

### 9강
1. 엔트로피(entropy)는 다음 식과 같이 정의된다.
![[Pasted image 20241206224352.png]]
동전의 앞면(H)과 뒷면(T)이 나올 확률분포를 나타낸 다음 경우 중 가장 엔트로피가 높은 경우는?

(1) H: 0.5, T: 0.5
(2) H: 0.9, T: 0.1
(3) H: 1, T: 0
(4) H: 0, T: 1

정답: (1)

해설: 엔트로피의 정의는 H(X) = - ∑ p(x) log₂ p(x) 이다. 이 때, 확률분포가 균등할수록(모든 사건의 확률이 같을수록) 불확실성이 최대가 되어 엔트로피가 최대가 된다. 즉, 동전의 경우 앞면과 뒷면이 정확히 반반의 확률(0.5, 0.5)일 때 엔트로피가 가장 크다.

2. 다음 LDA(Latent Dirichlet Allocation) 관련 설명 중 적절하지 않은 것은?

(1) 문서에서 토픽을 뽑아내기 위한 방법이다.
(2) 학습시 토픽의 수를 hyperparameter로 주어야 한다.
**(3) 토픽은 문서들의 확률분포로 표현된다.**
(4) LDA 모델의 파라미터는 Gibbs sampling을 통해 학습할 수 있다.

정답: (3)

해설: LDA(Latent Dirichlet Allocation)는 텍스트 코퍼스(여러 문서 집합) 안에 숨어있는 ‘주제(Topic)’를 찾아내는 확률적 모델이다.  

간단히 말해, 각 문서가 여러 주제의 혼합물(칵테일)이라고 가정하고, 각 주제는 특정 단어들을 자주 포함하는 경향이 있다고 본다. LDA는 문서에 등장하는 단어들을 보고, 그 뒤 문서가 어떤 주제들로 이루어졌고 각 주제가 어떤 단어들로 구성되는지를 추론한다.

쉽게 예를 들어보자. 신문 기사들을 모아놓은 경우를 생각해보면, 어떤 문서들은 “스포츠”, “정치”, “경제” 등의 주제를 섞어서 다룰 수 있다. LDA는 아무런 ‘주제’에 대한 레이블 없이도 문서들을 분석해, “이 문서는 스포츠 관련 단어들이 70% 정도 포함되고, 정치 관련 단어들이 20%, 경제 관련 단어들이 10% 정도 섞여 있구나”와 같은 식으로 문서에 내재한 주제 비율과 각 주제별로 어떤 단어들이 중요하게 쓰이는지 알아낸다.

**토픽은 단어들의 확률분포로 표현됩니다.**

3. LDA(Latent Dirichlet Allocation) 모델의 학습 관련된 다음 설명 중 적절하지 않은 것은?

(1) 학습이 진행됨에 따라 perplexity 값이 감소하게 된다.
(2) Gibbs sampling을 통해 추출된 샘플 중 초기에 추출된 일부 샘플은 버리게 된다.
(3) 낮은 perplexity가 사람이 이해하기 좋은 토픽 형성을 보장하는 것은 아니다.
**(4) Topic coherence 값이 낮을수록 학습이 잘 되었다고 볼 수 있다.**

정답: (4)

   Perplexity(혼란도)는 모델이 새로운 데이터(단어 시퀀스)를 얼마나 잘 예측하는지를 나타내는 지표다. LDA 모델을 학습하면, 모델이 텍스트 데이터를 점차 잘 설명할 수 있게 되고, 그에 따라 perplexity 값은 일반적으로 감소하는 경향을 보인다. 즉, 학습이 잘 진행되면 모델이 데이터에 덜 '놀라고', 예측이 더 좋아지므로 perplexity가 내려간다. 이 설명은 적절하다.

   LDA 모델 학습 시 Gibbs Sampling과 같은 마코프 체인 몬테카를로(MCMC) 기법을 사용한다면, 체인이 초기 상태에서 수렴하지 않은 상태일 수 있다. 따라서 초기 burn-in 기간 동안의 샘플은 폐기하는 것이 일반적이다. 이 설명은 적절하다.

   Perplexity는 모델의 통계적 성능(예측력)을 보는 것이지, 사람이 봤을 때 납득하기 좋은 해석 가능한 토픽이 나오는지를 직접 보장하지 않는다. 낮은 perplexity가 좋은 해석력을 반드시 의미하는 것은 아니므로, 이 설명도 타당하다.
   
Topic coherence(토픽 일관성)는 토픽 내 단어들의 의미적, 상관적 일관성을 측정하는 지표다. 일반적으로 토픽 일관성이 높을수록 사람이 이해하기 좋은 토픽이라고 할 수 있다. 즉, coherence 값이 높으면 “토픽 내 단어들이 의미적으로 잘 묶여 있다”는 것을 의미하고, 이는 “학습이 더 잘 된 토픽”이라고 해석할 수 있다. 따라서 coherence 값이 낮을수록 학습이 잘 되었다고 보는 것은 적절치 않다.


### 10강
1. 다음 중 RNN(Recurrent Neural Network)에 대한 설명으로 적절하지 않은 것은?

(1) 시퀀스(sequence)를 다루기 위한 neural network의 일종이다.
(2) hidden state를 통해 이전 시점의 입력으로부터 추출된 정보가 이후 시점으로 전달된다.
**(3) 시퀀스 길이가 길어져도 되면 정보를 전달하는 과정에서 정보 손실이 일어나지 않는 특징이있다.**
(4) 이전 시점 t-1의 hidden state로부터 다음 시점 t의 hidden state를 계산하기 위한 파라미터들은시점 t와 상관 없이 모두 같은 파라미터가 사용된다.

정답: (3)

해설: 시퀀스 길이가 길어지면 되면 정보를 전달하는 과정에서 상대적으로 시퀀스 앞쪽의 정보손실이 일어나게 됩니다. 이와 같은 문제를 해결하고자 LSTM 또는 GRU가 등장하게 되었습니다.

2. 다음은 LSTM의 구조를 나타내는 그림이다. 관련된 설명으로 적절하지 않은 것은?
![[Pasted image 20241206230615.png]]
(1) RNN과 달리 ℎ𝑡 , 𝑐𝑡같이 두 개의 상태를 유지하고 있다.
(2) 𝑖𝑡 , 𝑓 𝑡 와 같은 gate들을 사용하여 어떤 입력값을 통과시키고 어떤 기억은 잊어버릴지를 제어하
고 있다.
![[Pasted image 20241206230730.png]]
(4) tanh 함수를 적용하게 되면 -1에서 1 사이의 값이 만들어진다.

정답: (3)

해설: element-wise product(⊗)를 적용하면 같은 위치의 원소끼리 곱하게 되므로 결과는 [0
0]이 됩니다.

3. 다음 그림이 나타내고 있는 RNN 구조에 대한 설명으로 적절하지 않은 것은?
![[Pasted image 20241206230831.png]]
(1) 같은 𝑊를 사용해 얼마든지 unfold 할 수 있기 때문에, prediction 시에는 학습 때 사용되었던시퀀스 길이와 상관 없이 임의의 길이를 가진 시퀀스에 대해 출력을 만들어낼 수 있다.
(2) 출력 𝑜(𝑡)와 레이블 𝑦(𝑡)를 가지고 loss 𝐿(𝑡)를 계산하여 학습한다.
(3) ℎ(𝑡) 벡터에 다음 시점으로 전달할 정보가 담겨 있다.
**(4) 시퀀스를 입력으로 받아 하나의 벡터를 만들어내는 RNN 구조이다.**

정답: (4)

해설: 매 시점마다 출력 𝑜(𝑡)를 만들어내고 있기 때문에 시퀀스를 받아 시퀀스를 만들어내는 RNN
구조를 나타내고 있습니다.

### 11강
1. 다음은 주어진 문자열 다음에 올 문자를 예측하는 방식으로 텍스트를 생성해내는 모델을 표현
한 그림과, 모델의 입력부터 출력까지의 흐름을 구현한 코드이다. 이 모델에 대한 설명으로 적절
하지 않은 것은?
![[Pasted image 20241206231223.png]]
![[Pasted image 20241206231254.png]]
(1) GRU 셀을 이용하여 구현하였다.
(2) 입력 x는 Embedding Layer, GRU, Dense Layer를 거쳐 최종적으로 logit 값들을 만들어내게 된다.
(3) GRU 셀은 GRU Output과 state에 해당하는 두 개의 벡터를 출력으로 만들어낸다.
**(4) 최종적으로 단어들에 대한 확률을 만들어내기 위해 logit 값들에 대해 softmax 함수를 적용하고 있다.**

정답: (4)

해설: call 함수 내부를 보면 dense layer를 적용해서 나온 logit 값들을 그대로 리턴하고 있는 것을
볼 수 있습니다. 따라서 이 코드로 보면 softmax 함수는 적용하지 않았습니다.

2. 다음은 영화 리뷰 텍스트를 입력으로 받아 영화에 대한 감정이 긍정인지 부정인지를 분류하는 모델의 구조와 코드를 나타낸 것이다. 모델 관련 설명으로 적절하지 않은 것은?
![[Pasted image 20241206231418.png]]
![[Pasted image 20241206231442.png]]
(1) TextVectorization layer는 단어에 해당하는 index 값을 출력으로 만들어낸다.
(2) 단어 하나에 대응하는 임베딩(embedding) 벡터의 차원 수는 64이다.
(3) 각각의 Bidirectional RNN으로부터 나오는 hidden state 벡터의 차원은 64이다.
**(4) 텍스트 하나 당 최종적으로 vocabulary 수만큼의 logit 값을 만들어낸다.**

정답: (4)

해설: 코드를 보면 최종적으로 하나의 logit 값을 만들어내는 것을 볼 수 있습니다.
최종 Dense 레이어가 Dense(1)로 되어 있으므로 텍스트 하나당 최종 출력은 단 하나의 스칼라(logit)이다. 따라서 vocabulary 수 만큼의 logit 값을 만드는 것이 아니며, 이는 분류를 위해 출력 차원이 1(이진 분류)인 것을 감안할 때 완전히 틀린 설명이다.

3. 괄호 안에 들어갈 단어로 적절한 것은?

"( )은/는 문장들의 시퀀스를 같은 길이로 맞춰주기 위해 가장 긴 문장의 길이를 기준으로해서, 짧은 문장의 경우 남는 자리를 0으로 채워주는 것이다.

**(1) padding**
(2) masking
(3) embedding
(4) compile

정답: (1)

### 12강
1. 다음은 번역 태스크를 수행하기 위한 모델의 구조를 나타낸 것이다. 관련 설명으로 적절하지
않은 것은?
![[Pasted image 20241206231916.png]]
(1) Encoder와 Decoder로 이루어져 있다.
(2) Encoder의 입력으로 번역의 대상이 되는 문장이 들어가게 된다.
(3) C는 Encoder에 입력으로 들어온 문장의 정보를 축약하고 있는 벡터이다.
**(4) t 시점에서 Decoder의 hidden state ℎ𝑡 는 벡터 C와 t-1 시점의 hidden state인 ℎ𝑡−1 에 의해서만 결정되고 있다.**

정답: (4)

해설: 그림을 보면 ℎ𝑡는 C와 ℎ𝑡−1 외에도 𝑦𝑡−1에 의해서 결정되고 있는 것을 알 수 있습니다.

2. Attention에 대한 다음 설명 중 적절하지 않은 것은?

**(1) Encoder-Decoder 구조에서 Attention 메커니즘을 적용하게 되면, encoder의 마지막 hidden state를 context vector로 사용하게 된다.**
(2) Luong attention의 경우 score 함수로 dot product를 사용하게 되면, decoder의 hidden state와 encoder의 hidden state 사이의 dot product 값이 클수록 context vector 생성시에 더 큰 가중치를 부여하게 된다.
(3) Attention을 적용한 모델은, Attention을 적용하지 않은 Encoder-Decoder 모델에 비해 상대적으로 더 긴 시퀀스를 용이하게 다룰 수 있다.
(4) Attention의 직관적인 의미는 현 시점에서 상대적으로 더 중요한 부분에 주목하겠다는 것이다.

정답: (1)

해설: Encoder-Decoder 구조에서 Attention 메커니즘을 적용하게 되면, 무조건 encoder의 마지막 hidden state를 context vector로 사용하는 것이 아니라 encoder의 모든 hidden state들을 고려해서 context vector를 만들게 됩니다.

3. 다음은 주어진 문장을 번역하는 Encoder-Decoder 모델을 나타낸 그림과 코드이다. 모델에 관한 설명으로 적절하지 않은 것은?
![[Pasted image 20241206232230.png]]
![[Pasted image 20241206232245.png]]
(1) 코드에서 self.encoder에 입력으로 들어가는 context는 번역의 대상이 되는 문장을 가리킨
다고 할 수 있다.
(2) Encoder는 입력으로 들어온 토큰들과 같은 수의 벡터들을 생성하여 Decoder의 입력으로 넘겨
주고 있다.
(3) Encoder에는 Bidirectional RNN이 사용된 것으로 보인다.
**(4) self.decoder에 입력으로 들어가는 context는 그림에서 [START] What time is it 과 같은 시퀀스에 해당하는 부분이라고 할 수 있다.**

정답: (4)

해설: 코드를 보면 self.decoder에 입력으로 들어가는 context는 Encoder에서 출력된 값임을
알 수 있습니다.
Bidirectional RNN은 문장의 양쪽 방향에서 정보를 모아 현재 단어를 더 잘 이해하는 방식이라고 할 수 있다.


### 13강
1. 다음은 Transformer(Self-Attention)와 다른 모델의 여러가지 복잡도를 비교한 표이다. 관련 설명 중 적절하지 않은 것은?
![[Pasted image 20241206232640.png]]
(1) 시퀀스 길이 𝑛이 커질 때, 모델 내 최대 경로의 길이는 Transformer가 RNN에 비해 상대적으로 더 짧다고 볼 수 있다.
(2) RNN은 시퀀스 길이 𝑛이 커지게 되면 순차적으로 수행해야 하는 연산의 수가 선형적으로 비례
해서 늘어난다.
(3) Transformer의 경우에는 시퀀스 길이 𝑛과 순차적으로 수행해야 하는 연산의 수가 독립적이기
때문에 𝑛과 상관 없이 병렬적인 방식으로 학습이 가능하다.
**(4) RNN이 Transformer보다 더 긴 시퀀스에 대해 잘 학습할 수 있다.**

정답: (4)

해설: 표를 해석해 보면 대체로 RNN은 시퀀스 길이 𝑛이 늘어남에 따라 학습에 불리한 부분들이
커지게 됩니다. 따라서 Transformer가 RNN보다 더 긴 시퀀스에 대해 잘 학습할 수 있다고 볼 수있습니다.

2. 최초로 제안된 Transformer 모델과 관련한 다음 설명 중 적절하지 않은 것은?

(1) Encoder-Decoder 구조로 이루어져 있다.
(2) 단어들의 위치 정보를 추가해 주기 위해 positional encoding을 사용하였다.
**(3) Encoder 부분에서는 masked multi-head attention을 사용한다.**
(4) self-attention 부분은 value들의 가중 평균으로 계산되는데, 가중치 계산에 query와 key를 사용 하게 된다.

정답: (3)
해설: masked multi-head attention은 Encoder가 아니라 Decoder 부분에서 사용됩니다.

3. 다음이 Transformer의 layer normalization 부분을 나타내는 코드일 때 < > 자리에 들어갈 함수로 적절한 것은?
![[Pasted image 20241206232926.png]]
(1) min
(2) max
(3) var
**(4) mean**

정답: (4)

해설: layer normalization 부분에서는 평균을 빼고 표준편차로 나눠주는 standardization 과정을 적용하기 때문에 mean 함수가 오는 것이 적절합니다.


### 14강
1. 다음 ( ) 안에 들어갈 단어로 가장 적절한 것은?
pretrained model에 실제로 해결하고자 하는 타겟 태스크 분야의 데이터셋을 주고 학습시키는 과
정을 가리키는 용어는 ( )이다.

(1) gradient descent
(2) overfit
**(3) fine-tuning**
(4) dropout

정답: (3)

2. 다음 중 ULMFiT 논문에서 학습시 모델 weight의 변화량을 통제하기 위해 사용한 기법이 아닌
것은?

(1) Discriminative Fine-Tuning
(2) Slanted Triangular Learning Rates
**(3) Dropout**
(4) Gradual Unfreezing

정답: (3)
해설 : ULMFiT(Universal Language Model Fine-tuning) 

1. **Discriminative Fine-Tuning (판별적 미세 조정)**: 모델의 각 층에 대해 다른 학습률을 적용하여, 상위 층은 더 큰 학습률을, 하위 층은 더 작은 학습률을 사용함으로써 효과적으로 미세 조정을 수행합니다.
2. **Slanted Triangular Learning Rates (슬랜티드 트라이앵귤러 학습률)**: 학습 초반에 학습률을 점차 증가시켰다가 이후에 감소시키는 학습률 스케줄을 사용하여 모델이 안정적으로 수렴하도록 돕습니다.
3. **Dropout (드롭아웃)**: 일반적으로 과적합을 방지하기 위한 정규화 기법으로 사용됩니다. 하지만 ULMFiT 논문에서 모델 가중치의 변화량을 직접적으로 통제하기 위한 기법으로는 사용되지 않았습니다.
4. **Gradual Unfreezing (점진적 동결 해제)**: 모델의 층을 하나씩 동결 해제하면서 미세 조정을 진행하여, 초기에는 상위 층부터 학습시키고 점차 하위 층을 학습시킵니다.


3. 다음 그림은 Transformer의 구조 중 일부를 나타내고 있다. 다음 Keras 코드 (1)~(4) 중 그림
에서 붉은색으로 표시한 부분을 구현하는 코드로 가장 적절한 것은?
![[Pasted image 20241206233431.png]]
![[Pasted image 20241206233451.png]]
![[Pasted image 20241206233507.png]]
![[Pasted image 20241206233516.png]]
![[Pasted image 20241206233524.png]]
정답: (3)

해설: 그림이 Decoder의 masked multi-head attention 부분을 나타내고 있으므로,
use_causal_mask = True 와 같은 옵션을 주어서 mha 함수를 호출하는 것이 적절합니다.

### 15강
1. 다음 ( ) 안에 들어갈 단어들의 쌍으로 가장 적절한 것은?
BERT는 Transformer의 ( ) 부분만 쌓아올려 만든 모델인 반면, GPT는 Transformer의 ( ) 부분 만 사용하여 구현한 것이다.

(1) Decoder, Encoder
(2) Decoder, Feed Forward Network
**(3) Encoder, Decoder**
(4) Feed Forward Network, Encoder

정답: (3)

2. 다음 ( ) 안에 들어갈 단어들의 쌍으로 가장 적절한 것은?
GPT -2는 별도의 ( ) 없이 ( )만으로 여러가지 자연언어처리 태스크를 수행할 수 있는 가능성을 보여주었다.

(1) pretraining, fine-tuning
(2) language model, fine-tuning
**(3) fine-tuning, language model**
(4) Decoder, Encoder

정답: (3)


3. 다음 ( ) 안에 들어갈 단어들의 쌍으로 가장 적절한 것은?
GPT -3는 더 긴 시퀀스를 다룰 수 있게 하기 위해 ( ) attention과 ( ) attention을 번갈아 사용하였다.

(1) Bahdanau, Luong
(2) dot-product, concatenative
**(3) dense, sparse**
(4) cross, global

정답: (3)